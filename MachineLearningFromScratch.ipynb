{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qfyr0buIxQjX"
   },
   "source": [
    "# **CS 5361 Machine Learning**\n",
    "**Final Project: Machine Learning from Scratch**\n",
    "\n",
    "**Author:**  Robert Alvarez<br>\n",
    "\n",
    "**Last modified:** Nov 18th  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wky81iDgDCxo"
   },
   "source": [
    "Modules use for project development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylcdujB83Kd2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import numpy.typing as npt\n",
    "from typing import Tuple\n",
    "from nptyping import NDArray, Shape, Float, Integer, Number, Obj\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from abc import ABC, abstractmethod\n",
    "from time import time\n",
    "from math import pi\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8GZUiRwiMTN"
   },
   "source": [
    "Upload data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "mAfcs1cJiNwj",
    "outputId": "74f566e1-000b-4e65-d97c-5e4f2f9ed184"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-2cb4271a-4bfd-4a1a-b6e7-1714c8ec3d89\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-2cb4271a-4bfd-4a1a-b6e7-1714c8ec3d89\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Datasets.zip to Datasets.zip\n"
     ]
    }
   ],
   "source": [
    "upload = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CGSTHhQiYe4"
   },
   "outputs": [],
   "source": [
    "!unzip -q Datasets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BjeltUL2lP4J",
    "outputId": "fb24d6b5-39a1-4760-f421-3d6ad3ddba4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets.zip\t gpu_running_time.csv  particles_y.npy\n",
      "doh_dataset.csv  __MACOSX\t       Pecan.txt\n",
      "gamma04.txt\t particles_X.npy       sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPArrayNxM = NDArray[Shape[\"N, M\"], Obj]\n",
    "\n",
    "NumNPArrayNxM = NDArray[Shape[\"N, M\"], Number]\n",
    "NumNPArray = NDArray[Shape[\"N\"], Number]\n",
    "\n",
    "FloatNPArrayNxM = NDArray[Shape[\"N, M\"], Float]\n",
    "FloatNPArrayNxN = NDArray[Shape[\"N, N\"], Float]\n",
    "FloatNPArray = NDArray[Shape[\"N\"], Float]\n",
    "\n",
    "IntNPArrayNxM = NDArray[Shape[\"N, M\"], Integer]\n",
    "IntNPArray = NDArray[Shape[\"N\"], Integer]\n",
    "\n",
    "ArrayLike = npt.ArrayLike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXhlKNgDwdep"
   },
   "source": [
    "## **Performance Metrics**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6Z8eB9swaR_"
   },
   "source": [
    "Precision$(y,p) = \\frac{TP}{TP+FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLah7P6fwy62"
   },
   "outputs": [],
   "source": [
    "def precision(y: NumNPArray, p: NumNPArray) -> np.float64:\n",
    "  \"\"\"\n",
    "  Calculate precision between ground truth and estimates.\n",
    "\n",
    "  Args:\n",
    "      y (NumNPArray): Ground truth (correct) target values.\n",
    "      p (NumNPArray): Estimated targets as returned by a classifier.\n",
    "\n",
    "  Returns:\n",
    "      np.float64: Ratio between the True Positives and all the points that are classified as Positives.\n",
    "  \"\"\"\n",
    "  return np.sum(y*p)/np.sum(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6VWHuQ-w_Te"
   },
   "source": [
    "Recall$(y,p) = \\frac{TP}{TP+FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WG3cJb8vw_oX"
   },
   "outputs": [],
   "source": [
    "def recall(y: NumNPArray, p: NumNPArray) -> np.float64:\n",
    "  \"\"\"\n",
    "  Calculate recall between ground truth and estimates.\n",
    "\n",
    "  Args:\n",
    "      y (NumNPArray): Ground truth (correct) target values.\n",
    "      p (NumNPArray): Estimated targets as returned by a classifier.\n",
    "\n",
    "  Returns:\n",
    "      np.float64: Measure of our model correctly identifying True Positives.\n",
    "  \"\"\"\n",
    "  return np.sum(y*p)/np.sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cW9yo_gBzWFN"
   },
   "source": [
    "$\n",
    "    F1 = \\frac{2*Precision(y,p)*Recall(y,p)}{Precision(y,p)+Recall(y,p)}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9hxomJVxmeT"
   },
   "outputs": [],
   "source": [
    "def f1(y: NumNPArray, p: NumNPArray) -> np.float64:\n",
    "  \"\"\"\n",
    "  Calculate f1 score between ground truth and estimates.\n",
    "\n",
    "  Args:\n",
    "      y (NumNPArray): Ground truth (correct) target values.\n",
    "      p (NumNPArray): Estimated targets as returned by a classifier.\n",
    "\n",
    "  Returns:\n",
    "      np.float64: Harmonic mean of the Precision and Recall.\n",
    "  \"\"\"\n",
    "  pr = precision(y,p)\n",
    "  r = recall(y,p)\n",
    "  return 2*pr*r/(pr+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEmrzoybxnl2"
   },
   "source": [
    "Accuracy$(y,p) = \\frac{TP+TN}{TP+FN+TN+FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhrUhdhm24k9"
   },
   "outputs": [],
   "source": [
    "def accuracy(y: NumNPArray, p: NumNPArray) -> np.float64:\n",
    "  \"\"\"\n",
    "  Calculate accuracy between ground truth and estimates.\n",
    "\n",
    "  Args:\n",
    "      y (NumNPArray): Ground truth (correct) target values.\n",
    "      p (NumNPArray): Estimated targets as returned by a classifier.\n",
    "\n",
    "  Returns:\n",
    "      np.float64: Fraction of correctly classified samples.\n",
    "  \"\"\"\n",
    "  return np.sum(y==p)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v78L3PCp26pi"
   },
   "source": [
    "Specificity$(y,p) = \\frac{TN}{TN+FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TaPE0Sbm3LxZ"
   },
   "outputs": [],
   "source": [
    "def specificity(y: NumNPArray, p: NumNPArray) -> np.float64:\n",
    "  \"\"\"\n",
    "  Calculate specificity between ground truth and estimates.\n",
    "\n",
    "  Args:\n",
    "      y (NumNPArray): Ground truth (correct) target values.\n",
    "      p (NumNPArray): Estimated targets as returned by a classifier.\n",
    "\n",
    "  Returns:\n",
    "      np.float64: Recall of the negative class.\n",
    "  \"\"\"\n",
    "  return np.sum((1-y)*(1-p))/np.sum(y==p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGCRgI3m3s4s"
   },
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzOcewtLxBo2"
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(y: NumNPArray, p: NumNPArray) -> list[list[int]]:\n",
    "  \"\"\"\n",
    "  Compute confusion matrix to evaluate the accuracy of a classification.\n",
    "\n",
    "  Args:\n",
    "      y (NumNPArray): Ground truth (correct) target values.\n",
    "      p (NumNPArray): Estimated targets as returned by a classifier.\n",
    "\n",
    "  Returns:\n",
    "      list[list[int]]: Confusion matrix whose i-th row and j-th column entry indicates the number of samples with true label being i-th class and predicted label being j-th class.\n",
    "  \"\"\"\n",
    "  y_u = np.unique(y)\n",
    "  m = []\n",
    "  for i in y_u:\n",
    "    p_i = p[y==i]\n",
    "    arr = []\n",
    "    for j in y_u:\n",
    "      arr.append(np.sum(p_i==j))\n",
    "    m.append(arr)\n",
    "  return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4q0eLvin3M5h"
   },
   "source": [
    "\\begin{equation}\n",
    "MAE=\\frac{1}{n}\\sum_{i=0}^{n}|Y_i-\\hat{Y_i}|\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNA7kbRr3fIO"
   },
   "outputs": [],
   "source": [
    "def MAE(y: NumNPArray, p: NumNPArray) -> np.float64:\n",
    "  \"\"\"\n",
    "  Calculate Mean Absolute Error between ground truth and estimates.\n",
    "\n",
    "  Args:\n",
    "      y (NumNPArray): Ground truth (correct) target values.\n",
    "      p (NumNPArray): Estimated targets as returned by a classifier.\n",
    "\n",
    "  Returns:\n",
    "      np.float64: Average of all output errors is returned.\n",
    "  \"\"\"\n",
    "  return np.sum(np.absolute(y-p))/y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxLVbw9a3kmJ"
   },
   "source": [
    "\\begin{equation}\n",
    "MSE=\\frac{1}{n}\\sum_{i=0}^{n}\\big(Y_i-\\hat{Y_i}\\big)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IU8WkGhw3qWO"
   },
   "outputs": [],
   "source": [
    "def MSE(y: NumNPArray, p: NumNPArray) -> np.float64:\n",
    "  \"\"\"\n",
    "  Calculate Mean Square Error between ground truth and estimates.\n",
    "\n",
    "  Args:\n",
    "      y (NumNPArray): Ground truth (correct) target values.\n",
    "      p (NumNPArray): Estimated targets as returned by a classifier.\n",
    "\n",
    "  Returns:\n",
    "      np.float64: Average of all output square errors is returned.\n",
    "  \"\"\"\n",
    "  return np.sum((y-p)**2)/y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTAaDpCGxqBL"
   },
   "source": [
    "## **Utils**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o70IR-Dhxqgf"
   },
   "source": [
    "The funciton to_categorical(y) recieve a vector y of size n, it find the k unique values in y from 0 to k-1 and it makes a n-by-k matrix where $y_{i,j} = 1$ if $y_i = j$ and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9VTvdL95LXK"
   },
   "outputs": [],
   "source": [
    "def to_categorical(y: IntNPArray) -> IntNPArray:\n",
    "  y_vals = np.unique(y)\n",
    "  cat_y = np.empty(shape=(y.shape[0],y_vals.shape[0]))\n",
    "  for i,y_val in enumerate(y_vals):\n",
    "    cat_y[:,i] = (y==y_val).astype(int)\n",
    "  return cat_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-EHHN4p5LzE"
   },
   "source": [
    "The function standardize(x) calculates $\\frac{x-\\mu}{\\sigma}$ for each column in x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azaG0Fnf5MHV"
   },
   "outputs": [],
   "source": [
    "def standardize(x: FloatNPArrayNxM) -> FloatNPArrayNxM:\n",
    "  for i in range(x.shape[1]):\n",
    "    x[:,i] = (x[:,i] - np.mean(x[:,i]))/np.std(x[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBBugNWr5Tsb"
   },
   "source": [
    "LebelEncoder is a class that label targest with values from 0 to k-1, where k is the number unique values. Fit find the k unique values on the data, transform change the targest values to the respective encode number and inverse_transform converts the numbers to the map values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "om1LuqA9xq2m"
   },
   "outputs": [],
   "source": [
    "class LabelEncoder:\n",
    "  \"\"\"\n",
    "  Class use to encode y values from 0 to n_classes-1, where n_classes is the\n",
    "  number of unique y values.\n",
    "  ...\n",
    "  Attributes\n",
    "  ----------\n",
    "  classes_ : numpy array of shape (n_classes,)\n",
    "    classes_ is a n_classes entries numpy array where each entry have a unique\n",
    "    y value\n",
    "\n",
    "   Methods\n",
    "  ---------\n",
    "  fit(data)\n",
    "    Find the k unique values on the data and save them in classes_ attribute.\n",
    "  transform(data)\n",
    "    Transform change the targest values to the respective encode number.\n",
    "  inverse_transform(data)\n",
    "    Reverse the transformer by mapping the numbers from 0 to n_classes to its\n",
    "    original value.\n",
    "  \"\"\"\n",
    "  # Constructor\n",
    "  def __init__(self) -> None:\n",
    "    self.classes_ = np.empty((0))\n",
    "\n",
    "  def fit(self, data: ArrayLike) -> None:\n",
    "    \"\"\"\n",
    "    Find the k unique values on the data and save them in classes_ attribute.\n",
    "\n",
    "    Args:\n",
    "        data (ArrayLike): _description_\n",
    "    \"\"\"\n",
    "    self.classes_ = np.unique(data)\n",
    "\n",
    "  def fit_transform(self, data: ArrayLike) -> IntNPArray:\n",
    "    #Get the classes if they haven't being set\n",
    "    if self.classes_.shape[0] == 0:\n",
    "      self.fit(data)\n",
    "    return self.transform(data)\n",
    "\n",
    "  def transform(self, data: ArrayLike) -> IntNPArray:\n",
    "    \"\"\"\n",
    "    It use classes_ to encode each data value to the respective index in the array.\n",
    "\n",
    "    Args:\n",
    "        data (ArrayLike): _description_\n",
    "\n",
    "    Returns:\n",
    "        IntNPArray: _description_\n",
    "    \"\"\"\n",
    "    if isinstance(data, list):\n",
    "      data = np.array(data, self.classes_.dtype)\n",
    "    temp = np.empty(data.shape,np.int32)\n",
    "    for i,c in enumerate(self.classes_):\n",
    "      temp[c==data] = i\n",
    "    return temp\n",
    "\n",
    "  def inverse_transform(self, data: IntNPArray) -> ArrayLike:\n",
    "    \"\"\" Replace each data values to what is in classes_ by using each data value as an index to classes_.\n",
    "\n",
    "    Args:\n",
    "        data (IntNPArray): _description_\n",
    "\n",
    "    Returns:\n",
    "        ArrayLike: _description_\n",
    "    \"\"\"\n",
    "    if isinstance(data, list):\n",
    "      shape = len(data)\n",
    "    else:\n",
    "      shape = data.shape\n",
    "    temp = np.empty(shape,self.classes_.dtype)\n",
    "    for i,c in enumerate(self.classes_):\n",
    "      i = np.int32(i)\n",
    "      temp[i==data] = c\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcNxX2fI5P5l"
   },
   "source": [
    "$relu(x) = max(0,x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzC6JgWD5QKv"
   },
   "outputs": [],
   "source": [
    "def relu(x: FloatNPArray) -> FloatNPArray:\n",
    "  \"\"\"\n",
    "  Return the maximum number between 0 and x for each value of x.\n",
    "\n",
    "  Args:\n",
    "      x (FloatNPArray): _description_\n",
    "\n",
    "  Returns:\n",
    "      FloatNPArray: Maximum number between 0 and x for each value of x.\n",
    "  \"\"\"\n",
    "  return np.maximum(np.array([[0]]),x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1Mx8uwu5RYT"
   },
   "source": [
    "$sigmoid(z) = \\frac{1}{1+exp(-z)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeW1OpZO5Rpw"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x: FloatNPArray) -> FloatNPArray:\n",
    "  \"\"\"\n",
    "  Returns 1/(1+exp(-z)) for each value of z.\n",
    "\n",
    "  Args:\n",
    "      x (FloatNPArray): _description_\n",
    "\n",
    "  Returns:\n",
    "      FloatNPArray: 1/(1+exp(-z)) for each value of z.\n",
    "  \"\"\"\n",
    "  return 1./(1.+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e51eV4mTxCLS"
   },
   "source": [
    "## **Data set processsing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3r04z7cgxYUO"
   },
   "outputs": [],
   "source": [
    "#Binary classification\n",
    "def process_gamma_dataset():\n",
    "  #Read file information\n",
    "  infile = open(\"gamma04.txt\",\"r\")\n",
    "  x, y  = [], []\n",
    "  for line in infile:\n",
    "    y.append(int(line[-2:-1] == 'g'))\n",
    "    x.append(np.fromstring(line[:-2], dtype=float, sep=','))\n",
    "  infile.close()\n",
    "\n",
    "  #Make dependent and independent numpy arrays\n",
    "  x = np.array(x).astype(np.float32)\n",
    "  y = np.array(y)\n",
    "\n",
    "  #Split the data\n",
    "  return train_test_split(x, y, test_size=0.2, random_state=4361)\n",
    "\n",
    "#Regression\n",
    "def process_gpu_running_time():\n",
    "  df = pd.read_csv('gpu_running_time.csv')\n",
    "  data = df.to_numpy()\n",
    "  X = data[:,:15]\n",
    "  y = np.mean(data[:,15:],axis=1)\n",
    "  return train_test_split(X, y, test_size=0.3, random_state=4361)\n",
    "\n",
    "#Binary classification\n",
    "def process_doh():\n",
    "  # Drop rows that have a NaN in them\n",
    "  df = pd.read_csv(\"doh_dataset.csv\", compression='gzip').dropna()\n",
    "  # Remove non-float data as that will mess up the student's code\n",
    "  df = df.drop(['TimeStamp', 'SourceIP', 'DestinationIP'])\n",
    "  # Extract the labels from the dataframe and encode them to integers\n",
    "  df_labels = df.pop('Label')\n",
    "  label_encoder = LabelEncoder()\n",
    "  df_labels = label_encoder.fit_transform(df_labels)\n",
    "\n",
    "  # Prepare arrays to split into training and testing sets\n",
    "  x_features = df.values\n",
    "  y_labels = np.array(df_labels).T\n",
    "\n",
    "  # Split into training (70%) and testing (30%)\n",
    "  return train_test_split(x_features, y_labels, train_size=0.7, random_state=1738, shuffle=True)\n",
    "\n",
    "#Multi-classification \n",
    "def mnist():\n",
    "  import tensorflow as tf\n",
    "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "  x_train = np.float32(x_train.reshape(x_train.shape[0],-1)/255)\n",
    "  x_test = np.float32(x_test.reshape(x_test.shape[0],-1)/255)\n",
    "  return x_train,x_test,y_train,y_test\n",
    "\n",
    "#Regression\n",
    "def process_particles():\n",
    "  PX = np.load('particles_X.npy')\n",
    "  Py = np.load('particles_y.npy')\n",
    "  return train_test_split(PX, Py, test_size=0.2,  random_state=4361)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMkU4rtdBubm"
   },
   "outputs": [],
   "source": [
    "#Save mnist dataset\n",
    "mnist_x_train, mnist_x_test, mnist_y_train, mnist_y_test = mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7g5gemZxBvW"
   },
   "source": [
    "## **Nearest Neighbor**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCVXKjx70Bhw"
   },
   "source": [
    "NearestNeighbor is a super class use by NearestNeighborClassifier and NearestNeighborRegressor.\n",
    "\n",
    "We denote the set of k nearest neighbor of $\\mathbf{x}$ as $S_\\mathbf{x}$. Where $S_\\mathbf{x}$ is defined as $S_\\mathbf{x} \\subseteq D$ s.t. $|S_\\mathbf{x}|=k$ and $\\forall (\\mathbf{x}',y') \\in D \\backslash S_\\mathbf{x}$,\n",
    "\\begin{equation}\n",
    "  dist(\\mathbf{x},\\mathbf{x}') \\ge \\max_{{(\\mathbf{x}'',y'') \\in S_\\mathbf{x}}} dist(\\mathbf{x},\\mathbf{x}'')\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJ52JMjMyc9l"
   },
   "outputs": [],
   "source": [
    "class NearestNeighbor(ABC):\n",
    "  \"\"\"\n",
    "  Abstract class use by KNeighborsClassifier and KNeighborsRegressor.\n",
    "  ...\n",
    "  Atributes\n",
    "  ---------\n",
    "  k : int\n",
    "    Number of nearest neighbors to be find when predicting\n",
    "  weighted : boolean\n",
    "    True for weighted and false for unweighted distance calculations.\n",
    "\n",
    "   Methods\n",
    "  ---------\n",
    "  fit(x_train, y_train, regression=False)\n",
    "    Save x_train and y_train regardless of regression boolean value. If regression=False\n",
    "    the unique y values are save in classes_ as another attribute of the class.\n",
    "  get_closest_k(x_test)\n",
    "    Calculate and return the k nearest neighbors for each x_test base on the x_train\n",
    "    values saved when calling fit.\n",
    "  \"\"\"\n",
    "  # Constructor\n",
    "  def __init__(self, k: int = 1, weighted: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Save parameters k=1 and weighted=False if no parameter are specified.\n",
    "\n",
    "    Args:\n",
    "        k (int, optional): Number of nearest neighbor to be consider for predictions. Defaults to 1.\n",
    "        weighted (bool, optional): True for weighted and false for unweighted distance calculations. Defaults to False.\n",
    "    \"\"\"\n",
    "    self.k = k\n",
    "    self.weighted = weighted\n",
    "\n",
    "  def fit(self, x_train: NumNPArrayNxM, y_train: ArrayLike, regression: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Save x_train and y_train regardless of regression boolean value. If regression=False\n",
    "    the unique y values are save in classes_ as another attribute of the class.\n",
    "\n",
    "    Args:\n",
    "        x_train (NumNPArrayNxM): _description_\n",
    "        y_train (ArrayLike): _description_\n",
    "        regression (bool, optional): _description_. Defaults to False.\n",
    "    \"\"\"\n",
    "    self.x_train = x_train\n",
    "    self.y_train = y_train\n",
    "    if not regression:\n",
    "      self.classes_ = np.unique(y_train)\n",
    "\n",
    "  def get_closest_k(self, x_test: NumNPArrayNxM) -> NumNPArray:\n",
    "    \"\"\"\n",
    "    Calculate and return the k nearest neighbors for each x_test base on the x_train\n",
    "    values saved when calling fit.\n",
    "\n",
    "    Args:\n",
    "        x_test (NumNPArrayNxM): _description_\n",
    "\n",
    "    Returns:\n",
    "        NumNPArray: _description_\n",
    "    \"\"\"\n",
    "    # Improved version using the fact that (a-b)**2 = a**2 - 2ab + b**2\n",
    "    dist1 = np.sum(x_test**2,axis=1,keepdims=True)\n",
    "    dist2 = -2.*np.matmul(x_test,self.x_train.T)\n",
    "    # This part is not really necessary, since it does not depend on x_test\n",
    "    dist3 = np.sum(self.x_train.T**2,axis=0,keepdims=True)\n",
    "    dist = dist1+dist2+dist3\n",
    " \n",
    "    # Distance matrix is created, get the k closest elements\n",
    "    return np.argpartition(dist, kth=self.k, axis=-1)[:,:self.k], dist\n",
    "\n",
    "  @abstractmethod\n",
    "  def predict(self, x_test):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75sUONLk0D1y"
   },
   "source": [
    "### **Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UGA5oob11Ly"
   },
   "source": [
    "With the same definition for $S_x$ we can define the classifier p() for unweighted distance as:\n",
    "\\begin{equation}\n",
    "  p(\\mathbf{x}) = mode(\\{y'' : (\\mathbf{x}'',y'') \\in S_\\mathbf{x}\\})\n",
    "\\end{equation}\n",
    "and for weighted distance as:\n",
    "\\begin{equation}\n",
    "  p(\\mathbf{x}) = \\underset{c \\in C}{\\mathrm{argmin}} \\sum_{{(\\mathbf{x}'',y'') \\in S_\\mathbf{x}}} \\frac{1}{dist(\\mathbf{x},\\mathbf{x}_i'')} \\delta_{c,y_i''}\n",
    "\\end{equation}\n",
    "where $C$ is the set off all possible $y \\in D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-I03l2KyHCb"
   },
   "outputs": [],
   "source": [
    "class KNeighborsClassifier(NearestNeighbor):\n",
    "  \"\"\"\n",
    "  Extension of NearestNeighbor to be use as a classifier. Classification is done\n",
    "  by chossing the most common target value from the k nearest neighbors.\n",
    "  \"\"\"\n",
    "  # Predict class of test data\n",
    "  def predict(self, x_test: NumNPArrayNxM) -> NumNPArray:\n",
    "    \"\"\"\n",
    "    Get k nearest neighbors by calling get_closest_k from the super class with\n",
    "    x_test as parameter. For unweighted distance, use those indeces to the k closes\n",
    "    elements to get the mode out of the y value of those indeces. For weighted\n",
    "    distance, use those indeces to get the target values and predict the maximum\n",
    "    argument of the class with largest number where the number of each class\n",
    "    is calculates by adding 1/dist(x,x'') to the class where x'' belongs to.\n",
    "\n",
    "    Args:\n",
    "        x_test (NumNPArrayNxM): _description_\n",
    "\n",
    "    Returns:\n",
    "        NumNPArray: _description_\n",
    "    \"\"\"\n",
    "    # Distance matrix is created, get the k closest elements\n",
    "    minIdxs, dist = super().get_closest_k(x_test)\n",
    "\n",
    "    # Build weight array with the votes and choose the one with the highest one to predict\n",
    "    possibles = np.zeros((dist.shape[0],self.classes_.shape[0]))\n",
    "    for irow,rowIdx in enumerate(minIdxs):\n",
    "      for idx in rowIdx:\n",
    "        if self.weighted:\n",
    "          possibles[irow,self.y_train[idx]] += 1/dist[irow,idx]\n",
    "        else:\n",
    "          possibles[irow,self.y_train[idx]] += 1\n",
    "\n",
    "    return self.classes_[np.argmax(possibles, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVPfitLz4eOE"
   },
   "source": [
    "Classification test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L85WAhcC3zGA",
    "outputId": "d923512c-19c3-4630-85fc-e3219091527c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1 weighted = False\n",
      "My model:\n",
      "\tElapse time = 25.42723\n",
      "\tAccuracy = 0.9691\n",
      "SKLEARN model:\n",
      "\tElapse time = 40.06176\n",
      "\tAccuracy = 0.9691\n",
      "\n",
      "k = 3 weighted = False\n",
      "My model:\n",
      "\tElapse time = 27.20054\n",
      "\tAccuracy = 0.9705\n",
      "SKLEARN model:\n",
      "\tElapse time = 40.52692\n",
      "\tAccuracy = 0.9705\n",
      "\n",
      "k = 5 weighted = False\n",
      "My model:\n",
      "\tElapse time = 27.93530\n",
      "\tAccuracy = 0.9688\n",
      "SKLEARN model:\n",
      "\tElapse time = 40.48159\n",
      "\tAccuracy = 0.9688\n",
      "\n",
      "k = 7 weighted = False\n",
      "My model:\n",
      "\tElapse time = 27.73766\n",
      "\tAccuracy = 0.9694\n",
      "SKLEARN model:\n",
      "\tElapse time = 40.21772\n",
      "\tAccuracy = 0.9694\n",
      "\n",
      "k = 9 weighted = False\n",
      "My model:\n",
      "\tElapse time = 27.98494\n",
      "\tAccuracy = 0.9659\n",
      "SKLEARN model:\n",
      "\tElapse time = 41.28986\n",
      "\tAccuracy = 0.9659\n",
      "\n",
      "k = 11 weighted = False\n",
      "My model:\n",
      "\tElapse time = 36.57435\n",
      "\tAccuracy = 0.9668\n",
      "SKLEARN model:\n",
      "\tElapse time = 57.19568\n",
      "\tAccuracy = 0.9668\n",
      "\n",
      "k = 13 weighted = False\n",
      "My model:\n",
      "\tElapse time = 35.17777\n",
      "\tAccuracy = 0.9653\n",
      "SKLEARN model:\n",
      "\tElapse time = 40.59555\n",
      "\tAccuracy = 0.9653\n",
      "\n",
      "k = 15 weighted = False\n",
      "My model:\n",
      "\tElapse time = 29.50026\n",
      "\tAccuracy = 0.9633\n",
      "SKLEARN model:\n",
      "\tElapse time = 40.54684\n",
      "\tAccuracy = 0.9633\n",
      "\n",
      "k = 1 weighted = True\n",
      "My model:\n",
      "\tElapse time = 25.00432\n",
      "\tAccuracy = 0.9691\n",
      "SKLEARN model:\n",
      "\tElapse time = 46.01315\n",
      "\tAccuracy = 0.9691\n",
      "\n",
      "k = 3 weighted = True\n",
      "My model:\n",
      "\tElapse time = 29.57863\n",
      "\tAccuracy = 0.9717\n",
      "SKLEARN model:\n",
      "\tElapse time = 45.27968\n",
      "\tAccuracy = 0.9717\n",
      "\n",
      "k = 5 weighted = True\n",
      "My model:\n",
      "\tElapse time = 27.76249\n",
      "\tAccuracy = 0.9691\n",
      "SKLEARN model:\n",
      "\tElapse time = 39.32206\n",
      "\tAccuracy = 0.9691\n",
      "\n",
      "k = 7 weighted = True\n",
      "My model:\n",
      "\tElapse time = 28.07322\n",
      "\tAccuracy = 0.9700\n",
      "SKLEARN model:\n",
      "\tElapse time = 40.33126\n",
      "\tAccuracy = 0.9700\n",
      "\n",
      "k = 9 weighted = True\n",
      "My model:\n",
      "\tElapse time = 28.71203\n",
      "\tAccuracy = 0.9676\n",
      "SKLEARN model:\n",
      "\tElapse time = 40.21096\n",
      "\tAccuracy = 0.9673\n",
      "\n",
      "k = 11 weighted = True\n",
      "My model:\n",
      "\tElapse time = 27.77281\n",
      "\tAccuracy = 0.9679\n",
      "SKLEARN model:\n",
      "\tElapse time = 40.04987\n",
      "\tAccuracy = 0.9678\n",
      "\n",
      "k = 13 weighted = True\n",
      "My model:\n",
      "\tElapse time = 27.62171\n",
      "\tAccuracy = 0.9667\n",
      "SKLEARN model:\n",
      "\tElapse time = 39.92363\n",
      "\tAccuracy = 0.9665\n",
      "\n",
      "k = 15 weighted = True\n",
      "My model:\n",
      "\tElapse time = 28.26236\n",
      "\tAccuracy = 0.9652\n",
      "SKLEARN model:\n",
      "\tElapse time = 39.94801\n",
      "\tAccuracy = 0.9647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNNC\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  x_train,x_test,y_train,y_test = mnist_x_train, mnist_x_test, mnist_y_train, mnist_y_test\n",
    "\n",
    "  for weighted in [False,True]:\n",
    "    weight = 'distance' if weighted else 'uniform'\n",
    "    for k in range(1,16,2):\n",
    "      print('k =',k,'weighted =',weighted)\n",
    "      print(\"My model:\")\n",
    "      t0 = time()\n",
    "      model = KNeighborsClassifier(k=k, weighted=weighted)\n",
    "      model.fit(x_train, y_train)\n",
    "      pred = model.predict(x_test)\n",
    "      print(\"\\tElapse time = {:.5f}\".format(time() - t0))\n",
    "      print('\\tAccuracy = {:6.4f}'.format(accuracy(y_test, pred)))\n",
    "\n",
    "      print(\"SKLEARN model:\")\n",
    "      t0 = time()\n",
    "      model = KNNC(n_neighbors=k, weights=weight)\n",
    "      model.fit(x_train, y_train)\n",
    "      pred = model.predict(x_test)\n",
    "      print(\"\\tElapse time = {:.5f}\".format(time() - t0))\n",
    "      print('\\tAccuracy = {:6.4f}\\n'.format(accuracy(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85ZHxl6O0E14"
   },
   "source": [
    "### **Regressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peqV9QE714aK"
   },
   "source": [
    "With the same definition for $S_x$ we can define our regressor $\\hat{y}$ as:\n",
    "\\begin{equation}\n",
    "  \\hat{y}(\\mathbf{x}) = \\frac{1}{k} \\sum_{i=1}^k \\frac{y_i''}{w_i}\n",
    "\\end{equation}\n",
    "where $w_i = 1$ for unweigthed distance and $w_i = dist(\\mathbf{x},\\mathbf{x}_i'')$ for weighted distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sgHcEjIsGKG"
   },
   "outputs": [],
   "source": [
    "class KNeighborsRegressor(NearestNeighbor):\n",
    "  \"\"\"\n",
    "  Extension of NearestNeighbor to be use as a regressor. Regression is done by\n",
    "  getting the k closest point at taking the average of each dimension.\n",
    "  \"\"\"\n",
    "  def predict(self, x_test: NumNPArrayNxM) -> NumNPArray:\n",
    "    \"\"\"\n",
    "    Prediction is done with the closest k points by taking the average of each\n",
    "    dimension for unweighted distance and the average with the weight of\n",
    "    1/dist(x,x'') for weighted distance.\n",
    "\n",
    "    Args:\n",
    "        x_test (NumNPArrayNxM): _description_\n",
    "\n",
    "    Returns:\n",
    "        NumNPArray: _description_\n",
    "    \"\"\"\n",
    "    # Distance matrix is created, get the k closest elements\n",
    "    minIdxs, dist = super().get_closest_k(x_test)\n",
    "\n",
    "    if self.weighted:\n",
    "      weights = np.array([1/dist[irow,rowIdx] for irow,rowIdx in enumerate(minIdxs)])\n",
    "      ans = np.average(self.y_train[minIdxs],axis=1,weights=weights)\n",
    "    else:\n",
    "      ans = np.mean(self.y_train[minIdxs],axis=1)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSd1dEto4cE1"
   },
   "source": [
    "Regressor test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jL0mZTVNmA9O",
    "outputId": "af258c5f-6012-4bb4-a149-eb7c4205c427"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pecan.txt for regression:\n",
      "k = 1\n",
      "My model:\n",
      "\tPrediction =  [543.79796 310.     ]\n",
      "\tElapse time = 0.01567\n",
      "SKLEARN model:\n",
      "\tPrediction =  [543.79796 310.     ]\n",
      "\tElapse time = 0.00375\n",
      "\n",
      "k = 3\n",
      "My model:\n",
      "\tPrediction =  [534.9925781  313.71552503]\n",
      "\tElapse time = 0.00113\n",
      "SKLEARN model:\n",
      "\tPrediction =  [534.9925781  313.71552503]\n",
      "\tElapse time = 0.00281\n",
      "\n",
      "k = 5\n",
      "My model:\n",
      "\tPrediction =  [530.5904805  319.46238006]\n",
      "\tElapse time = 0.00098\n",
      "SKLEARN model:\n",
      "\tPrediction =  [530.5904805  319.46238006]\n",
      "\tElapse time = 0.00233\n",
      "\n",
      "k = 1\n",
      "My model:\n",
      "\tPrediction =  [543.79796 310.     ]\n",
      "\tElapse time = 0.00126\n",
      "SKLEARN model:\n",
      "\tPrediction =  [543.79796 310.     ]\n",
      "\tElapse time = 0.00242\n",
      "\n",
      "k = 3\n",
      "My model:\n",
      "\tPrediction =  [535.60945781 313.24697075]\n",
      "\tElapse time = 0.00110\n",
      "SKLEARN model:\n",
      "\tPrediction =  [535.29663986 313.4814209 ]\n",
      "\tElapse time = 0.00223\n",
      "\n",
      "k = 5\n",
      "My model:\n",
      "\tPrediction =  [531.4747662  317.61443499]\n",
      "\tElapse time = 0.00103\n",
      "SKLEARN model:\n",
      "\tPrediction =  [531.02382964 318.52821935]\n",
      "\tElapse time = 0.00245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor as KNNR\n",
    "\n",
    "print(\"Using Pecan.txt for regression:\")\n",
    "#Read data\n",
    "df = pd.read_csv(\"Pecan.txt\", delimiter=\"\\t\")\n",
    "\n",
    "#Remove first and last column\n",
    "X = df.values[:, range(1, len(df.columns)-1)]\n",
    "Y = df.values[:, len(df.columns)-1]\n",
    "newData = np.array([[120,5,80], [20,40,15]])\n",
    "\n",
    "for weighted in [False,True]:\n",
    "  weight = 'distance' if weighted else 'uniform'\n",
    "  for k in range(1,6,2):\n",
    "    print('k =',k)\n",
    "    print(\"My model:\")\n",
    "    t0 = time()\n",
    "    model = KNeighborsRegressor(k=k,weighted=weighted)\n",
    "    model.fit(X,Y)\n",
    "    print(\"\\tPrediction = \", model.predict(newData))\n",
    "    print(\"\\tElapse time = {:.5f}\".format(time() - t0))\n",
    "\n",
    "    print(\"SKLEARN model:\")\n",
    "    t0 = time()\n",
    "    model = KNNR(n_neighbors=k,weights=weight)\n",
    "    model.fit(X,Y)\n",
    "    print(\"\\tPrediction = \", model.predict(newData))\n",
    "    print(\"\\tElapse time = {:.5f}\\n\".format(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ac5aBNaAw2G0"
   },
   "source": [
    "## **Linear Regression**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eXNRJqv09gE"
   },
   "source": [
    "Linear regression model tries to approximate $y$ as\n",
    "\\begin{equation}\n",
    "  \\hat{y} = \\beta_0 + \\sum_{j=1}^p \\mathbf{X}_j\\beta_j\n",
    "\\end{equation}\n",
    "\n",
    "giving a reduse sum of squares (RSS) of\n",
    "\\begin{align}\n",
    "  RSS(\\beta) &= \\sum_{i=1}^N (y_i - \\hat{y_i})^2 \\\\\n",
    "             &= \\sum_{i=1}^N \\big(y_i -\\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j\\big)^2\n",
    "\\end{align}\n",
    "\n",
    "Some calculus and matrix properties use:\n",
    "- $\\nabla_x (f(x)g(x)) = g(x)\\nabla_xf(x) + f(x)\\nabla_xg(x) \\text{ where } x \\in \\mathbb{R}^n \\text{ and } f(x),g(x): \\mathbb{R}^n \\rightarrow \\mathbb{R} $\n",
    "- $a,b \\in \\mathbb{R}^n, a^Tb = b^Ta \\text{ because } a^Tb \\text{ and } b^Ta \\text{ are scalar values.} $\n",
    "- $\\beta \\in \\mathbb{R}^n, \\frac{\\partial}{\\partial \\beta} \\beta^T = \\delta_{i,j} = \\mathbf{I}_n$\n",
    "\n",
    "By appending a column of 1's as the first column of $\\mathbf{x}$ we can rewrite $RSS(\\beta)$ as\n",
    "\\begin{align}\n",
    "  RSS(\\beta) &= (\\mathbf{y} - \\mathbf{X}\\beta)^T(\\mathbf{y} - \\mathbf{X}\\beta) \\\\\n",
    "  &= (\\mathbf{y}^T - \\beta^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\beta) \\\\\n",
    "  &= \\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\beta - \\beta^T\\mathbf{X}^T\\mathbf{y} + \\beta^T\\mathbf{X}^T\\mathbf{X}\\beta \\\\\n",
    "  &= \\mathbf{y}^T\\mathbf{y} - 2\\mathbf{y}^T\\mathbf{X}\\beta + \\beta^T\\mathbf{X}^T\\mathbf{X}\\beta\n",
    "\\end{align}\n",
    "\n",
    "To minimize this function, we would take the first derivative of $RSS$.\n",
    "\\begin{align}\n",
    "  \\frac{\\partial RSS}{\\partial \\beta} &= \\frac{\\partial}{\\partial \\beta}(\\mathbf{y}^T\\mathbf{y}) - 2\\frac{\\partial}{\\partial \\beta}(\\beta^T\\mathbf{X}^T\\mathbf{y}) + \\frac{\\partial}{\\partial \\beta}(\\beta^T\\mathbf{X}^T\\mathbf{X}\\beta) \\\\\n",
    "  &= 0 - 2\\mathbf{I}\\mathbf{X}^T\\mathbf{y} + \\frac{\\partial}{\\partial \\beta}(\\beta^T)\\mathbf{X}^T\\mathbf{X}\\beta + \\beta^T\\mathbf{X}^T\\mathbf{X} \\big(\\frac{\\partial}{\\partial \\beta}\\beta \\big) \\\\\n",
    "  &= -2\\mathbf{X}^T\\mathbf{y} + \\mathbf{I}\\mathbf{X}^T\\mathbf{X}\\beta + \\frac{\\partial}{\\partial \\beta}(\\beta^T) (\\beta^T\\mathbf{X}^T\\mathbf{X})^T \\\\\n",
    "  &= -2\\mathbf{X}^T\\mathbf{y} + \\mathbf{X}^T\\mathbf{X}\\beta + \\mathbf{X}^T\\mathbf{X}\\beta \\\\\n",
    "  &= -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\beta \\\\\n",
    "  &= -2\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\beta)\n",
    "\\end{align}\n",
    "\n",
    "For the second derivative we get\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial^2 RSS}{\\partial\\beta \\partial\\beta^T} = 2\\mathbf{X}^T\\mathbf{X}\n",
    "\\end{equation}\n",
    "\n",
    "Assuming that $\\mathbf{X}$ has full column rank, and hence $\\mathbf{X}^T\\mathbf{X}$ is positive definite, we set the first partial derivative to zero and solve for $\\beta$.\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial RSS}{\\partial \\beta} = -2\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\beta) = 0 \\\\\n",
    "  \\mathbf{X}^T\\mathbf{y} - \\mathbf{X}^T\\mathbf{X}\\beta = 0 \\\\\n",
    "  \\mathbf{X}^T\\mathbf{X}\\beta = \\mathbf{X}^T\\mathbf{y} \\\\\n",
    "  (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\beta = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\\\\n",
    "  \\hat{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "We can now calcualte $\\hat{y}$ as\n",
    "\\begin{equation}\n",
    "  \\hat{y} = \\mathbf{X}\\hat{\\beta} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ENduUAA0983"
   },
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "  \"\"\"\n",
    "  Module use for regression and classification. If used for classification,\n",
    "  a one-hot representation neeeds to be pass.\n",
    "  ...\n",
    "  Atributes\n",
    "  ---------\n",
    "  self.intercept_ : shape=(n_classes, )\n",
    "    Real value numbers that represent the bias of each class.\n",
    "  self.coef_ : shape=(n_classes, n_attributes, )\n",
    "    Real value numbers that represent the weights of each class.\n",
    "\n",
    "   Methods\n",
    "  ---------\n",
    "  fit(x_train, y_train)\n",
    "    Calcualte beta values as (X^T X)^-1 X^T Y and save them in self.intercept_\n",
    "    and self.coef_.\n",
    "  predict(x_test)\n",
    "    Use the self.intercept_ and self.coef_ to calculate the predicted values as\n",
    "    X(X^T X)^-1 X^T Y.\n",
    "  \"\"\"\n",
    "  def __init__(self) -> None:\n",
    "    pass\n",
    "\n",
    "  #(X.TX)^-1 X.T y\n",
    "  def fit(self, x_train: NumNPArrayNxM, y_train: ArrayLike) -> None:\n",
    "    \"\"\"\n",
    "    Use (X^T X)^-1 X^T Y to calculate the beta values.\n",
    "    \"\"\"\n",
    "    #Add column of ones to also calculate beta_0\n",
    "    X = np.hstack((np.ones(shape=(x_train.shape[0],1)),x_train))\n",
    "    ny_cols = 1 if len(y_train.shape) == 1 else y_train.shape[1]\n",
    "    y = np.reshape(y_train,(-1,ny_cols))\n",
    "\n",
    "    #Calculate beta coefficients\n",
    "    temp = np.linalg.pinv(np.matmul(X.T, X)) #(X.TX)^-1\n",
    "    temp = np.matmul(temp,X.T)\n",
    "\n",
    "    #Calculate linear regression for each column in y\n",
    "    all_betas = np.matmul(temp,y)\n",
    "    self.intercept_ = all_betas[0]\n",
    "    self.coef_ = all_betas[1:].T\n",
    "\n",
    "    if ny_cols == 1:\n",
    "      self.coef_ = np.reshape(self.coef_,(-1))\n",
    "      self.intercept_ = np.reshape(self.intercept_,(-1))\n",
    "\n",
    "  def predict(self, x_test: NumNPArrayNxM) -> NumNPArray:\n",
    "    \"\"\"\n",
    "    Calculate predicted values as X(X^T X)^-1 X^T Y.\n",
    "\n",
    "    Args:\n",
    "        x_test (NumNPArrayNxM): _description_\n",
    "\n",
    "    Returns:\n",
    "        NumNPArray: _description_\n",
    "    \"\"\"\n",
    "    ny_cols = self.intercept_.shape[0]\n",
    "    if ny_cols == 1:\n",
    "      pred = np.matmul(x_test,self.coef_) + self.intercept_\n",
    "    else:\n",
    "      pred = np.matmul(x_test,self.coef_.T) + self.intercept_\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hezj3hAG4N8p"
   },
   "source": [
    "Classification test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHdkjkWu8MBp"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8sM9Q6Mc4CuE",
    "outputId": "7edad882-1892-4d55-cb87-a5f1c9fe1900"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using mnist for classification:\n",
      "My model:\n",
      "Elapse time = 6.32316\n",
      "accuracy: 0.86030\n",
      "\n",
      "SKLEARN model:\n",
      "Elapse time = 4.82961\n",
      "accuracy: 0.86030\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUsing mnist for classification:\")\n",
    "x_train,x_test,y_train,y_test = mnist_x_train, mnist_x_test, mnist_y_train, mnist_y_test\n",
    "y_train_oh = to_categorical(y_train)\n",
    "\n",
    "print(\"My model:\")\n",
    "t0 = time()\n",
    "model = LinearRegression()\n",
    "model.fit(x_train,y_train_oh)\n",
    "pred = np.argmax(model.predict(x_test),axis=1)\n",
    "print(\"Elapse time = {:.5f}\".format(time() - t0))\n",
    "print(\"accuracy: {:.5f}\".format(accuracy(y_test, pred)))\n",
    "\n",
    "print(\"\\nSKLEARN model:\")\n",
    "t0 = time()\n",
    "model = LR()\n",
    "model.fit(x_train,y_train_oh)\n",
    "pred = np.argmax(model.predict(x_test),axis=1)\n",
    "print(\"Elapse time = {:.5f}\".format(time() - t0))\n",
    "print(\"accuracy: {:.5f}\".format(accuracy(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tbpy35Wt4MDg"
   },
   "source": [
    "Regression test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80aZEyBy4MYO",
    "outputId": "39db017c-2340-4e17-f411-5f04049e13c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using process_gpu_running_time for regression:\n",
      "My model:\n",
      "Elapse time = 0.05151\n",
      "MSE =  14.287121532509255\n",
      "\n",
      "SKLEARN model:\n",
      "Elapse time = 0.26056\n",
      "MSE =  14.287121532508602\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUsing process_gpu_running_time for regression:\")\n",
    "x_train,x_test,y_train,y_test = process_gpu_running_time()\n",
    "\n",
    "print(\"My model:\")\n",
    "t0 = time()\n",
    "model = LinearRegression()\n",
    "model.fit(x_train,y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"Elapse time = {:.5f}\".format(time() - t0))\n",
    "print(\"MSE = \", MSE(y_test, pred))\n",
    "\n",
    "print(\"\\nSKLEARN model:\")\n",
    "t0 = time()\n",
    "model = LR()\n",
    "model.fit(x_train,y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"Elapse time = {:.5f}\".format(time() - t0))\n",
    "print(\"MSE = \", MSE(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDGmDpqow52z"
   },
   "source": [
    "## **Logistic Regression**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSxMvqfY1DBM"
   },
   "source": [
    "Logistic regression model have the form\n",
    "\\begin{align}\n",
    "  log\\frac{Pr(G=1|X=x)}{Pr(G=K|X=x)} &= \\beta_{10} + \\beta_1^Tx \\\\\n",
    "  log\\frac{Pr(G=2|X=x)}{Pr(G=K|X=x)} &= \\beta_{20} + \\beta_2^Tx \\\\\n",
    "  &... \\\\\n",
    "  log\\frac{Pr(G=K-1|X=x)}{Pr(G=K|X=x)} &= \\beta_{(K-1)0} + \\beta_{K-1}^Tx\n",
    "\\end{align}\n",
    "\n",
    "We can solve for any probability from k=1 to K-1 and get the following equation\n",
    "\\begin{equation}\n",
    "  Pr(G=k|X=x) = \\frac{exp(\\beta_{k0} + \\beta_k^Tx)}{1+\\sum_{l=1}^{K-1} exp(\\beta_{l0} + \\beta_l^Tx)}, k=1,...,K-1\n",
    "\\end{equation}\n",
    "\n",
    "which makes the probability of the class K as follows\n",
    "\\begin{equation}\n",
    "  Pr(G=k|X=x) = \\frac{1}{1+\\sum_{l=1}^{K-1} exp(\\beta_{l0} + \\beta_l^Tx)}\n",
    "\\end{equation}\n",
    "\n",
    "To simplify notation we would rewrite $P(G=k|X=x)$ as $p_k(x;\\theta)$.\n",
    "The log likelihood of $\\theta$ is then\n",
    "\\begin{equation}\n",
    "  l(\\theta) = \\sum_{i=1}^N logp_{g_i}(x_i;\\theta)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kh6-2nckikaE"
   },
   "source": [
    "### **Binary Logistic Regression**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpX028Uois9z"
   },
   "source": [
    "For binary classification, we can encode the classes as 0/1 responses, where $y_i = 1$ whwne $g_i = 1$, and $y_i = 0$ where $g_i = 2$. Let $p_1(x;\\theta) = p(x;\\theta)$, and $p_2(x;\\theta) = 1 - p(x;\\theta)$. The log-likelihood can be written as\n",
    "\\begin{align}\n",
    "  l(\\beta) &= \\sum_{i=1}^N \\{y_ilog(p(x_i;\\beta)) + (1-y_i)log(1-p(x_i;\\beta))\\} \\\\\n",
    "  &= \\sum_{i=1}^N \\{y_i\\beta^Tx_i - log(1+exp(\\beta^Tx_i))\\}\n",
    "\\end{align}\n",
    "\n",
    "Some calculus properties:\n",
    "- $\\frac{d}{dx} log(f(x)) = \\frac{\\frac{d}{dx}f(x)}{f(x)}$\n",
    "- $\\frac{d}{dx} exp(f(x)) = f'(x) exp(f(x))$\n",
    "- Newton method: $x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)}$\n",
    "\n",
    "To minimize the log-likelihood we would have to find the first derivative with respect to \\beta to get\n",
    "\\begin{align}\n",
    "  \\frac{\\partial l(\\beta)}{\\partial\\beta} &= \\sum_{i=1}^N \\Big\\{y_i\\frac{\\partial}{\\partial\\beta}\\beta^Tx_i - \\frac{\\partial}{\\partial\\beta} log(1 + exp(\\beta^Tx_i)) \\Big\\} \\\\\n",
    "  \\frac{\\partial l(\\beta)}{\\partial\\beta} &= \\sum_{i=1}^N \\Big\\{ y_ix_i - \\frac{\\frac{\\partial}{\\partial\\beta}(1 + exp(\\beta^Tx_i))}{1 + exp(\\beta^Tx_i)} \\Big\\} \\\\\n",
    "  \\frac{\\partial l(\\beta)}{\\partial\\beta} &= \\sum_{i=1}^N \\Big\\{ y_ix_i - \\frac{x_iexp(\\beta^Tx_i)}{1 + exp(\\beta^Tx_i)} \\Big\\} \\\\\n",
    "  \\frac{\\partial l(\\beta)}{\\partial\\beta} &= \\sum_{i=1}^N x_i \\Big\\{ y_i - \\frac{exp(\\beta^Tx_i)}{1 + exp(\\beta^Tx_i)} \\Big\\} \\\\\n",
    "  \\frac{\\partial l(\\beta)}{\\partial\\beta} &= \\sum_{i=1}^N x_i \\big(y_i - p(x_i;\\beta) \\big) \\\\\n",
    "  \\frac{\\partial l(\\beta)}{\\partial\\beta} &= \\mathbf{X}^T(\\mathbf{y} - \\mathbf{p})\n",
    "\\end{align}\n",
    "\n",
    "Now for the second derivative we end up with\n",
    "\\begin{align}\n",
    "  \\frac{\\partial^2 l(\\beta)}{\\partial\\beta\\partial\\beta^T} &= -\\sum_{i=1}^N x_ix_i^Tp(x_i;\\beta)(1-p(x_i;\\beta)) \\\\\n",
    "  \\frac{\\partial^2 l(\\beta)}{\\partial\\beta\\partial\\beta^T} &= -\\mathbf{X}^T\\mathbf{W}\\mathbf{X} \\text{ where } \\mathbf{W}_{i,j} = p(x_i;\\beta)(1-p(x_i;\\beta)) \\text{ if } i=j \\text{, } 0 \\text{ otherwise.}\n",
    "\\end{align}\n",
    "\n",
    "Then we can use Newton method to solve the optimization problem\n",
    "$$\\min_{{\\beta \\in \\mathbb{R}^p}} \\hat{y}$$\n",
    "\\begin{align}\n",
    "  \\beta_{k+1} &= \\beta_k - \\Big(\\frac{\\partial^2 l(\\beta)}{\\partial\\beta\\partial\\beta^T}\\Big)^{-1}\\frac{\\partial l(\\beta_k)}{\\partial \\beta_k} \\\\\n",
    "  &= \\beta_k + (\\mathbf{X}^T\\mathbf{W}\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{y} - \\mathbf{p})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rz34KK5X1DQt"
   },
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression:\n",
    "  \"\"\"\n",
    "  Moduel use for binary classification task. Optimization use is Newton method.\n",
    "  ...\n",
    "  Atributes\n",
    "  ---------\n",
    "  self.intercept_ : shape=(1, )\n",
    "    Real value numbers that represent the bias of each class.\n",
    "  self.coef_ : shape=(n_attributes, )\n",
    "    Real value numbers that represent the weights of each class.\n",
    "\n",
    "   Methods\n",
    "  ---------\n",
    "  fit(x_train, y_train)\n",
    "    Calcualte intercept and beta coefficients using Newtons method.\n",
    "  predict(x_test)\n",
    "    Calculate predict values as sigmoid(XB)>0.5.\n",
    "  \"\"\"\n",
    "  def __init__(self, tol: float = 1e-4, max_iter: int = 100) -> None:\n",
    "    self.tol = tol\n",
    "    self.max_iter = max_iter\n",
    "\n",
    "  def fit(self, x_train: NumNPArrayNxM, y_train: ArrayLike, alpha: float = 0.001) -> None:\n",
    "    \"\"\"\n",
    "    Use Newton method for optimization of beta values. Formula to update beta is\n",
    "    beta_(i+1) = beta_i + (X^T WX)^-1 X^T (y-p)\n",
    "\n",
    "    Args:\n",
    "        x_train (NumNPArrayNxM): _description_\n",
    "        y_train (ArrayLike): _description_\n",
    "        alpha (float, optional): _description_. Defaults to 0.001.\n",
    "    \"\"\"\n",
    "    #Add a column of 1's at the beginning of the data to calculate y intercept\n",
    "    X = np.c_[np.ones((x_train.shape[0],1)),x_train]\n",
    "    #We have as many betas as we have attributes\n",
    "    betas = np.zeros((X.shape[1],1))\n",
    "    #Reshape y_train so it is a 1 column matrix\n",
    "    y = np.reshape(y_train,(-1,1))\n",
    "    for _ in range(self.max_iter):\n",
    "      p = sigmoid(np.dot(X,betas))\n",
    "      W = p*(1-p)\n",
    "      XTWX = np.matmul(X.T, W*X)\n",
    "      temp = np.matmul(np.linalg.inv(XTWX),X.T)\n",
    "      new_betas = betas + np.matmul(temp,(y-p))\n",
    "      if np.sum(abs(new_betas-betas)) < self.tol:\n",
    "        break\n",
    "      betas = new_betas\n",
    "    self.intercept_ = betas[0,0]\n",
    "    self.coef_ = betas[1:,0]\n",
    "\n",
    "  def predict(self, x_test: NumNPArrayNxM) -> NumNPArray:\n",
    "    \"\"\"\n",
    "    Predict y values as sigmoid(XB)>0.5.\n",
    "\n",
    "    Args:\n",
    "        x_test (NumNPArrayNxM): _description_\n",
    "\n",
    "    Returns:\n",
    "        NumNPArray: _description_\n",
    "    \"\"\"\n",
    "    X = np.c_[np.ones((x_test.shape[0],1)),x_test]\n",
    "    betas = np.hstack((np.reshape(self.intercept_,(-1)),self.coef_))\n",
    "    z = np.dot(X,betas)\n",
    "    return (sigmoid(z)>0.5).astype(int)\n",
    "\n",
    "  def __str__(self) -> str:\n",
    "    return '{}(max_iter={:03})'.format(self.__class__.__name__,self.max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vxAfswB4mpG"
   },
   "source": [
    "Binary classification test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "me95SyvjjXum"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "De_2VUhX4pwC",
    "outputId": "526e6e53-5434-49a5-f63b-b1f937e26a4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My model\n",
      "model =  BinaryLogisticRegression(max_iter=100, penalty=None)\n",
      "Elapse time = 0.022\n",
      "accuracy: 0.78759\n",
      "\n",
      "SKLEARN model\n",
      "model =  LogisticRegression(penalty=None)\n",
      "Elapse time = 0.04869\n",
      "accuracy: 0.78759\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = process_gamma_dataset()\n",
    "standardize(x_train)\n",
    "standardize(x_test)\n",
    "\n",
    "#My model\n",
    "print(\"My model\")\n",
    "t0 = time()\n",
    "model = BinaryLogisticRegression()\n",
    "print(\"model = \", model)\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"Elapse time = {:.3f}\".format(time() - t0))\n",
    "print(\"accuracy: {:.5f}\".format(accuracy(y_test, pred)))\n",
    "\n",
    "#Run sklearn\n",
    "print(\"\\nSKLEARN model\")\n",
    "t0 = time()\n",
    "model = LR(max_iter=100, penalty=None, tol=1e-4)\n",
    "print(\"model = \", model)\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"Elapse time = {:.5f}\".format(time() - t0))\n",
    "print(\"accuracy: {:.5f}\".format(accuracy(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UpNLl3FizF8"
   },
   "source": [
    "### **Multinomial Logistic Regression**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ugZBz70i1Xa"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "  def __P(self, X: NumNPArrayNxM, B: NumNPArray) -> NumNPArray:\n",
    "    N = X.shape[0]  #Number of object in training set\n",
    "    P = X.shape[1]\n",
    "    a = np.empty((N,self.K))\n",
    "    for i in range(self.K):\n",
    "      a[:,i] = np.reshape(np.exp(np.dot(X,B[i*P:i*P+P])),(-1))\n",
    "    a /= np.sum(a,axis=1).reshape((-1,1))\n",
    "    return np.reshape(a,(-1,1))\n",
    "    #return np.reshape(a,(-1,1),order='F')\n",
    "\n",
    "  def __init__(self, tol: float = 1e-4, max_iter: int = 100) -> None:\n",
    "    self.tol = tol\n",
    "    self.max_iter = max_iter\n",
    "\n",
    "  def fit(self, x_train: NumNPArrayNxM, y_train: ArrayLike, alpha: float = 0.001) -> None:\n",
    "    self.K = np.unique(y_train).shape[0]  #Number of classes\n",
    "    K = self.K\n",
    "    #Add a column of 1's at the beginning of the data to calculate y intercept\n",
    "    X = np.c_[np.ones((x_train.shape[0],1)),x_train]\n",
    "    N = X.shape[0]  #Number of object in training set\n",
    "    P = X.shape[1]  #Number of parameters + 1 because of the 1's column\n",
    "    #Reshape y_train so it is a 1 column matrix\n",
    "    Y = to_categorical(y_train).reshape((-1,1))\n",
    "    #Set beta values to 0\n",
    "    B = np.zeros(shape=(K*P,1))\n",
    "    new_B = np.empty(shape=(K*P,1))\n",
    "    for j in range(self.max_iter):\n",
    "      #Calculate the probabilities\n",
    "      p = np.empty((N,self.K))\n",
    "      for i in range(K):\n",
    "        #p[:,i] = np.reshape(np.exp(np.dot(X,B[i::K])),(-1))\n",
    "        p[:,i] = np.reshape(np.exp(np.dot(X,B[i*P:i*P+P])),(-1))\n",
    "      p /= np.sum(p,axis=1).reshape((-1,1))\n",
    "      p = np.reshape(p,(-1,1))\n",
    "      #Calculate new betas\n",
    "      y_p = Y-p\n",
    "      for i in range(K):\n",
    "        #W = p[i*N:i*N+N]\n",
    "        W = p[i::K]                                                                                                                                               \n",
    "        W = W*(1.-W)\n",
    "        XTWX = np.matmul(X.T,W*X)\n",
    "        temp = np.matmul(np.linalg.pinv(XTWX),X.T)\n",
    "        new_B[i*P:i*P+P] = B[i*P:i*P+P] + np.matmul(temp,y_p[i::K])\n",
    "        #new_B[i::K] = B[i::K] + np.matmul(temp,y_p[i::K])\n",
    "        #new_B[i::K] = B[i::K] + np.matmul(temp,y_p[i*N:i*N+N])\n",
    "        #new_B[i*P:i*P+P] = B[i*P:i*P+P] + np.matmul(temp,y_p[i*N:i*N+N])\n",
    "      diff = B.reshape((P,K)) - new_B.reshape((P,K))\n",
    "      print(np.all(np.sum(abs(diff)) < self.tol))\n",
    "      if np.all(np.sum(abs(diff)) < self.tol):\n",
    "        print(j)\n",
    "        break\n",
    "      B = new_B\n",
    "    self.intercept_ = np.array(B[0::P])\n",
    "    #Get all betas that are not the intercepts and store it as a 2D array, 1 row per class coefficients\n",
    "    self.coef_ = np.delete(B,list(range(0,B.shape[0],P)),axis=0).reshape((-1,P-1))\n",
    "\n",
    "  def predict(self, x_test: NumNPArrayNxM) -> NumNPArray:\n",
    "    X = np.c_[np.ones((x_test.shape[0],1)),x_test]\n",
    "    B = np.hstack((self.intercept_,self.coef_)).reshape((-1))\n",
    "    p = np.reshape(self.__P(X,B),(-1,self.K))\n",
    "    return np.argmax(p,axis=1)\n",
    "\n",
    "  def __str__(self) -> str:\n",
    "    return '{}(max_iter={:03})'.format(self.__class__.__name__,self.max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "po_Ey0kujGzy"
   },
   "source": [
    "Binary classification with multinomial logistic regression implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fy78yZL4jL1-",
    "outputId": "2f7db1a0-55b3-46c7-e559-9aeab1a93a84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My model\n",
      "model =  LogisticRegression(max_iter=100)\n",
      "False\n",
      "True\n",
      "1\n",
      "Elapse time = 0.018\n",
      "accuracy: 0.73843\n",
      "\n",
      "SKLEARN model\n",
      "model =  LogisticRegression(penalty=None)\n",
      "Elapse time = 0.14528\n",
      "accuracy: 0.79022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = process_gamma_dataset()\n",
    "#x_train,x_test,y_train,y_test = mnist()\n",
    "\n",
    "#standardize(x_train)\n",
    "#standardize(x_test)\n",
    "\n",
    "#My model\n",
    "print(\"My model\")\n",
    "t0 = time()\n",
    "model = LogisticRegression()\n",
    "print(\"model = \", model)\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"Elapse time = {:.3f}\".format(time() - t0))\n",
    "print(\"accuracy: {:.5f}\".format(accuracy(y_test, pred)))\n",
    "\n",
    "#Run sklearn simple model\n",
    "print(\"\\nSKLEARN model\")\n",
    "t0 = time()\n",
    "model = LR(max_iter=100, penalty=None, tol=1e-4)\n",
    "print(\"model = \", model)\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"Elapse time = {:.5f}\".format(time() - t0))\n",
    "print(\"accuracy: {:.5f}\".format(accuracy(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmwlluGSw6Q2"
   },
   "source": [
    "## **Naive Bayes**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJ8Y7VW616iy"
   },
   "source": [
    "Bayes theorem states $$ P(C|X) = \\frac{P(X|C)P(C)}{P(X)} $$ which can be use to predict a class C as \n",
    "\\begin{align}\n",
    "  \\hat{C} &= \\underset{c \\in C}{\\mathrm{argmax}} P(c|X) \\\\\n",
    "  &= \\underset{c \\in C}{\\mathrm{argmax}} \\frac{P(X|c)P(c)}{P(X)} \\\\\n",
    "  &\\propto \\underset{c \\in C}{\\mathrm{argmax}} P(X|c)P(c) \\\\\n",
    "  &= \\underset{c \\in C}{\\mathrm{argmax}} \\Pi_i P(X_i|c)P(c) \\\\\n",
    "  &\\propto \\underset{c \\in C}{\\mathrm{argmax}} log \\Big( P(c) + \\sum_i P(X_i|c) \\Big)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahdiJ9ji18KX"
   },
   "source": [
    "### **Bernoulli**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFk1cq6F2Gz6"
   },
   "source": [
    "For Bernoulli we calculate our conditional probabilities from binarize inputs as $$ P(X_i | y) = P(X_i=1 | y)x_i + (1-P(X_i=1) | y))(1-X_i) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QH4vsM4a2HGH"
   },
   "outputs": [],
   "source": [
    "class BernoulliNB:\n",
    "  \"\"\"\n",
    "  Module use to predict y values from binarize X data to calculate conditional\n",
    "  probabilities as P(y|X) = P(X=1|y)X + (1-P(X=1|y))(1-X).\n",
    "  ...\n",
    "  Atributes\n",
    "  ---------\n",
    "  self.binarize : float\n",
    "    Value use to binarize data. If None is given we assume data is already\n",
    "    binarized.\n",
    "  self.alpha : float\n",
    "    Value use as smoother in the conditional probabilities.\n",
    "  self.classes_ : shape=(n_classes, )\n",
    "    Numpy array with all classes sorted in ascending order.\n",
    "  self.pac1 : shape = (n_classes, n_attributes, )\n",
    "    2D numpy array with the conditional probabilities of such aattribute in\n",
    "    class being equal to 1.\n",
    "  self.pac0 : shape = (n_classes, n_attributes, )\n",
    "    Same as self.pac1 but for value 0.\n",
    "  self.class_prior_ : shape = (n_classes, )\n",
    "    Log probability for getting a class.\n",
    "\n",
    "   Methods\n",
    "  ---------\n",
    "  fit(x_train, y_train)\n",
    "    Calcualte the mean occurance of each class and the probability of each\n",
    "    attribute beeing 1 or zero given the class.\n",
    "  predict(x_test)\n",
    "    Using probability of each class and the the prbability of 1 or 0 for each\n",
    "    attribute in each class we predict using the following formula:\n",
    "    P(X|y) = P(X=1|y)X + (1-P(X=1|y))(1-X).\n",
    "  \"\"\"\n",
    "  def __init__(self, alpha: float = 1.0, force_alpha: bool = False, binarize: float | None = None) -> None:\n",
    "    self.binarize = binarize\n",
    "    if alpha is not None:\n",
    "      self.alpha = 1e-10 if force_alpha and alpha<1e-10 else alpha\n",
    "    else:\n",
    "      self.alpha = -1.0\n",
    "\n",
    "  def fit(self, x_train: NumNPArrayNxM, y_train: ArrayLike) -> None:\n",
    "    \"\"\"\n",
    "    Calcate the mean occurance of each class as np.mean(y_train==y) and then take\n",
    "    the log probability of each to calculate self.class_prior_. Also, calculate\n",
    "    self.pac1[i] as np.mean(x_train[y_train==y]) for each attribute.\n",
    "    \"\"\"\n",
    "    if self.binarize is not None:\n",
    "      x_train = (x_train>self.binarize).astype(int)\n",
    "\n",
    "    self.classes_ = np.sort(np.unique(y_train))\n",
    "    pc = np.empty(self.classes_.shape[0])\n",
    "    self.pac1 = np.empty((self.classes_.shape[0],x_train.shape[1]))\n",
    "\n",
    "    for i,y in enumerate(self.classes_):\n",
    "      # Identify which sample match this class\n",
    "      ind = y_train==y\n",
    "      # Calculate probability to get 1\n",
    "      self.pac1[i] = np.mean(x_train[ind],axis=0)\n",
    "      # Calculate average of how many samples match this class\n",
    "      pc[i] = np.mean(ind)\n",
    "\n",
    "    #Add smoothing and re-normalize probabilities\n",
    "    if self.alpha > 0.0:\n",
    "      pc += self.alpha # Add smoothing\n",
    "      self.pac1 = self.pac1*(1-2*self.alpha) + self.alpha\n",
    "    self.pac0 = 1-self.pac1\n",
    "    self.class_prior_ = np.log(pc)\n",
    "\n",
    "  def predict(self, x_test: NumNPArrayNxM) -> ArrayLike:\n",
    "    \"\"\"\n",
    "    We predict each class probability as 𝑙𝑜𝑔(𝑃(y)+∑𝑃(𝑋𝑖|y) where 𝑙𝑜𝑔(𝑃(y) is\n",
    "    self.class_prior_ and 𝑃(𝑋𝑖|y) = P(X=1|y)X + (1-P(X=1|y))(1-X).\n",
    "    \"\"\"\n",
    "    if self.binarize is not None:\n",
    "      x_test = (x_test>self.binarize).astype(int)\n",
    "\n",
    "    x_test_0 = 1.0-x_test\n",
    "    probs = np.empty((x_test.shape[0],self.classes_.shape[0]))\n",
    "    for i in range(self.classes_.shape[0]):\n",
    "      p = self.pac1[i:i+1]*x_test + self.pac0[i:i+1]*x_test_0\n",
    "      probs[:,i] = np.sum(np.log(p),axis=1)\n",
    "    return self.classes_[np.argmax(probs + self.class_prior_,axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmczTwW24zFz"
   },
   "source": [
    "Classification test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "881hduxrjcbK"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB as BNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sIKsLznT4zUN",
    "outputId": "9f34a3be-f457-4453-d2b2-8b54e175107a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My model:\n",
      "Elapse time = 2.87926\n",
      "accuracy: 0.84650\n",
      "\n",
      "SKLEARN model:\n",
      "Elapse time = 1.10330\n",
      "accuracy: 0.84700\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = mnist_x_train, mnist_x_test, mnist_y_train, mnist_y_test\n",
    "mean = np.mean(x_train)\n",
    "\n",
    "print(\"My model:\")\n",
    "t0 = time()\n",
    "model = BernoulliNB(alpha=1e-6,binarize=mean)\n",
    "model.fit(x_train,y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"Elapse time = {:.5f}\".format(time() - t0))\n",
    "print(\"accuracy: {:.5f}\".format(accuracy(y_test, pred)))\n",
    "\n",
    "print(\"\\nSKLEARN model:\")\n",
    "t0 = time()\n",
    "model = BNB(alpha=1e-6,binarize=mean)\n",
    "model.fit(x_train,y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"Elapse time = {:.5f}\".format(time() - t0))\n",
    "print(\"accuracy: {:.5f}\".format(accuracy(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzfuT7LT1gvN"
   },
   "source": [
    "### **Gaussian**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q31N_zNh17G_"
   },
   "source": [
    "We calculate the conditional probabilities as $$ P(X_i|y) = \\frac{1}{\\sigma_y\\sqrt{2\\pi}} exp\\bigg( -\\frac{(X_i-\\mu_y)^2}{2\\sigma_y^2} \\bigg)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNvakV6Q1hCd"
   },
   "outputs": [],
   "source": [
    "class GaussianNB:\n",
    "  \"\"\"\n",
    "  Model to predict y values by assuming that all attributes are normaly\n",
    "  distributed. We can calculate probabilities as\n",
    "  P(X|y)=1/(sigma*sqrt(2pi))*exp(-(X-mu_y)^2/2sigma^2)\n",
    "  ...\n",
    "  Atributes\n",
    "  ---------\n",
    "  self.epsilon_ : float\n",
    "    Smoother use on each self.var_ entry to avoid numerical error calculations.\n",
    "  self.classes_ : (n_classes, )\n",
    "    Numpy array with all classes sorted in ascending order.\n",
    "  self.var_ : (n_classes, n_attributes, )\n",
    "    Variance of each attribute for each class.\n",
    "  self.theta_ : (n_classes, n_attributes, )\n",
    "    Mean of each attribute for each class.\n",
    "  self.class_prior_ : (n_classes, )\n",
    "    Log probability for getting a class.\n",
    "\n",
    "   Methods\n",
    "  ---------\n",
    "  fit(x_train, y_train)\n",
    "    Calculate the mean occurance of each class, the variance and mean of each\n",
    "    attribute for each class.\n",
    "  predict(x_test)\n",
    "    Use gauss distribution assumption it calculate the conditional probability\n",
    "    and take the maximum argument from all probabilities.\n",
    "  \"\"\"\n",
    "  def __init__(self, var_smoothing: float = 1e-09) -> None:\n",
    "    self.epsilon_ = var_smoothing\n",
    "\n",
    "  def fit(self, x_train: NumNPArrayNxM, y_train: ArrayLike) -> None:\n",
    "    \"\"\"\n",
    "    Get all possible predict values, for each class it calculate the variance\n",
    "    and mean of each attribute.\n",
    "\n",
    "    Args:\n",
    "        x_train (NumNPArrayNxM): _description_\n",
    "        y_train (ArrayLike): _description_\n",
    "    \"\"\"\n",
    "    self.classes_ = np.sort(np.unique(y_train))\n",
    "    self.var_ = np.empty((self.classes_.shape[0],x_train.shape[1]))\n",
    "    self.theta_ = np.empty((self.classes_.shape[0],x_train.shape[1]))\n",
    "    pc = np.empty(self.classes_.shape[0])\n",
    "\n",
    "    for i,y in enumerate(self.classes_):\n",
    "      # Identify which sample match this class\n",
    "      ind = y_train==y\n",
    "      sub_x = x_train[ind]\n",
    "      self.theta_[i] = np.mean(sub_x,axis=0)\n",
    "      self.var_[i] = np.std(sub_x,axis=0) + self.epsilon_\n",
    "      pc[i] = np.mean(ind)\n",
    "    self.class_prior_ = np.log(pc)\n",
    "\n",
    "  def predict(self, x_train: NumNPArrayNxM) -> ArrayLike:\n",
    "    \"\"\"\n",
    "    We predict each class probability as log(P(y)+∑𝑃(Xi|y) where log(P(y) is\n",
    "    self.class_prior_ and P(Xi|y) = 1/(sigma*sqrt(2pi))*exp(-(X-mu_y)^2/2sigma^2)\n",
    "\n",
    "    Args:\n",
    "        x_train (NumNPArrayNxM): _description_\n",
    "\n",
    "    Returns:\n",
    "        ArrayLike: _description_\n",
    "    \"\"\"\n",
    "    np.seterr(divide = 'ignore')\n",
    "    probs = np.empty((x_test.shape[0],self.classes_.shape[0]))\n",
    "    t = 1.0/np.sqrt(2.0*pi)\n",
    "    for i in range(self.classes_.shape[0]):\n",
    "      p = (t/self.var_[i])*np.exp(-0.5*((x_test-self.theta_[i])/self.var_[i])**2)\n",
    "      probs[:,i] = np.sum(np.log(p),axis=1)\n",
    "    return self.classes_[np.argmax(probs + self.class_prior_,axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PIEfGlX6rN_"
   },
   "source": [
    "Classification test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liRG5P3AjfhA"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB as GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2HEPUJeD6snU",
    "outputId": "1268885f-c963-4751-8b7f-d585325e5ec6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My model:\n",
      "Elapse time = 3.60504\n",
      "accuracy: 0.52710\n",
      "\n",
      "SKLEARN model:\n",
      "Elapse time = 1.25368\n",
      "accuracy: 0.61510\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = mnist_x_train, mnist_x_test, mnist_y_train, mnist_y_test\n",
    "\n",
    "print(\"My model:\")\n",
    "t0 = time()\n",
    "model = GaussianNB(var_smoothing=1e-6)\n",
    "model.fit(x_train,y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"Elapse time = {:.5f}\".format(time() - t0))\n",
    "print(\"accuracy: {:.5f}\".format(accuracy(y_test, pred)))\n",
    "\n",
    "print(\"\\nSKLEARN model:\")\n",
    "t0 = time()\n",
    "model = GNB(var_smoothing=1e-6)\n",
    "model.fit(x_train,y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"Elapse time = {:.5f}\".format(time() - t0))\n",
    "print(\"accuracy: {:.5f}\".format(accuracy(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGWRTiG-w6wr"
   },
   "source": [
    "## **Decision Trees**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZJrQPAz2adS"
   },
   "outputs": [],
   "source": [
    "def best_feature_DT(x_train: NumNPArrayNxM, y_train: ArrayLike) -> int:\n",
    "  best_feature = -1\n",
    "  best_acc = -1.0\n",
    "  for i_feature in range(x_train.shape[1]):\n",
    "    sub_x = x_train[:,i_feature]\n",
    "    if isinstance(sub_x[0], (np.integer, np.floating)):\n",
    "      i_mean = np.mean(sub_x)\n",
    "      l_temp = calc_predict(y_train[sub_x<=i_mean])\n",
    "      r_temp = calc_predict(y_train[sub_x>i_mean])\n",
    "      curr_acc = (l_temp[1] + r_temp[1])/y_train.shape[0]\n",
    "    else:\n",
    "      curr_acc = 0.0 \n",
    "      for option in np.unique(sub_x):\n",
    "        curr_acc += calc_predict(y_train[sub_x==option])[1]\n",
    "      curr_acc /= y_train.shape[0]\n",
    "\n",
    "    if best_acc < curr_acc:\n",
    "      best_acc = curr_acc\n",
    "      best_feature = i_feature\n",
    "  return best_feature\n",
    "\n",
    "def calc_predict(y_train: ArrayLike) -> Tuple[int, int]:\n",
    "  #Get all possible values of y_train\n",
    "\n",
    "  #Calculate which options gives the highest accuracy\n",
    "  largest_sum = -1\n",
    "  best_option = -1\n",
    "  for option in np.unique(y_train):\n",
    "    curr_sum = np.sum(y_train==option)\n",
    "    if largest_sum < curr_sum:\n",
    "      largest_sum = curr_sum\n",
    "      best_option = option\n",
    "  return (best_option, largest_sum)\n",
    "\n",
    "class InvalidParameterError(Exception):\n",
    "  def __init__(self, messate: str = \"Invalid parameter\") -> None:\n",
    "    print(messate)\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "  def __init__(self, splitter: str = \"best\", max_depth: int | None = None) -> None:\n",
    "    #Decision tree cannot be a leaf unless it have a predicted value assigned\n",
    "    self.is_leaf = False\n",
    "    self.splitter = splitter\n",
    "    self.max_depth = max_depth\n",
    "\n",
    "  def split_feature_DT(self, x_train: NumNPArrayNxM, y_train: ArrayLike, feature_idx: int) -> None:\n",
    "    self.branches = dict()\n",
    "    self.feature_idx = feature_idx\n",
    "    #Get all rows for feature selected\n",
    "    sub_x = x_train[:,feature_idx]\n",
    "    #If the attribute is numeric we use the mean, if it is categorical we use the unique values\n",
    "    if isinstance(sub_x[0], (np.integer, np.floating)):\n",
    "      self.isnumeric = True\n",
    "      #Split using the mean\n",
    "      self.break_point = np.mean(sub_x)\n",
    "      for i in range(2):\n",
    "        new_depth = None if self.max_depth is None else self.max_depth-1\n",
    "        dt = DecisionTreeClassifier(splitter=self.splitter, max_depth=new_depth)\n",
    "        #Add dt into the tree and get the indices for the left or right branch\n",
    "        if i == 0:\n",
    "          new_idxs = sub_x <= self.break_point\n",
    "          self.branches[\"le\"] = dt\n",
    "        else:\n",
    "          new_idxs = sub_x > self.break_point\n",
    "          self.branches[\"g\"] = dt\n",
    "        #If max_depth is 1 or if only one y value is left, we create the leafs\n",
    "        new_y = y_train[new_idxs]\n",
    "        if self.max_depth == 1 or np.unique(new_y).shape[0]==1:\n",
    "          #Set dt as a leaf and fill in its attributes\n",
    "          dt.is_leaf = True\n",
    "          dt.pred_val = calc_predict(new_y)[0]\n",
    "        else:\n",
    "          dt.fit(x_train[new_idxs], new_y)\n",
    "    #If the attribute is categorical we use its different possible values\n",
    "    else:\n",
    "      self.isnumeric = False\n",
    "      #Get all possible classification values from sub_x\n",
    "      options = np.unique(sub_x)\n",
    "      #Make a branch for each option\n",
    "      for option in options:\n",
    "        new_depth = None if self.max_depth is None else self.max_depth-1\n",
    "        dt = DecisionTreeClassifier(splitter=self.splitter, max_depth=new_depth)\n",
    "        #Add dt as a branch                                                                                                                                                           \n",
    "        self.branches[option] = dt\n",
    "        new_idxs = sub_x==option\n",
    "        new_y = y_train[new_idxs]\n",
    "        #If max_depth is 1 or if only one y value is left, we create the leafs\n",
    "        if self.max_depth == 1 or np.unique(new_y).shape[0]==1:\n",
    "          #Set dt as a leaf and fill in its attributes\n",
    "          dt.is_leaf = True\n",
    "          dt.pred_val = calc_predict(new_y)[0]\n",
    "        else:\n",
    "          dt.fit(x_train[new_idxs], new_y)\n",
    "\n",
    "  def fit(self, x_train: NumNPArrayNxM, y_train: ArrayLike, feature_idx: int | None = None) -> None:\n",
    "    #If max_depth is less than 1 we fail\n",
    "    if self.max_depth is not None and self.max_depth < 1:\n",
    "      raise InvalidParameterError(\"max_depth must be in the range [1,inf)\")\n",
    "\n",
    "    #feature_idx is out of bound we fail\n",
    "    if feature_idx is not None and (feature_idx < 0 or feature_idx > x_train.shape[1]):\n",
    "      raise InvalidParameterError(f\"feature_idx must be in the range [0,{x_train.shape[1]}]\")\n",
    "\n",
    "    #Use feature index given or use splitter option to get one\n",
    "    if (feature_idx is None):\n",
    "      if self.splitter == \"best\":\n",
    "        best_feature = best_feature_DT(x_train, y_train)\n",
    "        self.split_feature_DT(x_train, y_train, best_feature)\n",
    "      elif self.splitter == \"random\":\n",
    "        self.split_feature_DT(x_train, y_train, np.random.randint(0, x_train.shape[1], 1)[0])\n",
    "    #Else use the feature column that is given\n",
    "    else:\n",
    "      self.split_feature_DT(x_train, y_train, feature_idx)\n",
    "\n",
    "  def predict(self, x_train: NumNPArrayNxM) -> ArrayLike:\n",
    "    #Make prediction array\n",
    "    pred = np.empty(shape=(x_test.shape[0]))\n",
    "\n",
    "    if self.is_leaf:\n",
    "      pred.fill(self.pred_val)\n",
    "      return pred\n",
    "\n",
    "    #Create sub_x with the column of interest\n",
    "    sub_x = x_test[:,self.feature_idx]\n",
    "\n",
    "    #For this simple case we only have leafs\n",
    "    if self.isnumeric:\n",
    "      #Do a recursive prediction for both branches\n",
    "      l_sub = sub_x<=self.break_point\n",
    "      pred[l_sub] = self.branches[\"le\"].predict(x_test[l_sub])\n",
    "      r_sub = sub_x > self.break_point\n",
    "      pred[r_sub] = self.branches[\"g\"].predict(x_test[r_sub])\n",
    "    else:                                                                                                                                                                             \n",
    "      for option,dt in self.branches.items():\n",
    "        match_x = sub_x==option\n",
    "        #Select x_test object that match this branch and assign prediction values\n",
    "        pred[match_x] = dt.predict(x_test[match_x])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ja2XPZC16wpj"
   },
   "source": [
    "Classification test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vGu1G98jigX"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzZUW1sZ6xxm",
    "outputId": "1229556d-f28a-4f54-b340-a055f29800c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My model\n",
      "Elapse time = 112.840\n",
      "a: model accuracy =  0.8294\n",
      "SKLEARN model:\n",
      "Elapse time = 12.224\n",
      "a: model accuracy =  0.8654\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = mnist_x_train, mnist_x_test, mnist_y_train, mnist_y_test\n",
    "\n",
    "print(\"My model\")\n",
    "t0 = time()\n",
    "model = DecisionTreeClassifier(splitter=\"best\", max_depth=10)\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"Elapse time = {:.3f}\".format(time() - t0))\n",
    "print(\"a: model accuracy = \", accuracy(y_test, pred))\n",
    "\n",
    "print(\"SKLEARN model:\")\n",
    "t0 = time()\n",
    "model = DTC(splitter=\"best\", max_depth=10)\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"Elapse time = {:.3f}\".format(time() - t0))\n",
    "print(\"a: model accuracy = \", accuracy(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkZqHFzLw7Di"
   },
   "source": [
    "## **Dense Neural Network**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gU7LFzof21md"
   },
   "outputs": [],
   "source": [
    "class MLPRegressor:\n",
    "  def __init__(\n",
    "    self,\n",
    "    hidden_layer_sizes: Sequence[int] = (100,),\n",
    "    activation: str = 'relu',\n",
    "    solver: str = 'sgd',\n",
    "    learning_rate_init: float = 0.001,\n",
    "    momentum: float = 0.9\n",
    "  ) -> None:\n",
    "    self.hidden_layer_sizes = hidden_layer_sizes\n",
    "    self.act = activation\n",
    "    self.solv = solver\n",
    "    self.lern_rate = learning_rate_init\n",
    "    self.momentum = momentum\n",
    "\n",
    "  def fit(self, x_train: NumNPArrayNxM, y_train: ArrayLike) -> None:\n",
    "    #Create spaces for weights and biasses\n",
    "    self.coef_ = [np.random.randn(x_train.shape[1],self.hidden_layer_sizes[0])*np.sqrt(2./self.hidden_layer_sizes[0])]\n",
    "    self.intercepts_ = [np.zeros((self.hidden_layer_sizes[0]))]\n",
    "    change_W = [np.zeros(shape=self.coef_[0].shape)]\n",
    "    change_b = [np.zeros(shape=self.intercepts_[0].shape)]\n",
    "    for size in self.hidden_layer_sizes[1:]:\n",
    "      self.coef_.append(np.random.randn(self.coef_[-1].shape[1],size)*np.sqrt(2./self.hidden_layer_sizes[0]))\n",
    "      self.intercepts_.append(np.zeros((size)))\n",
    "      change_W.append(np.zeros(self.coef_[-1].shape))\n",
    "      change_b.append(np.zeros((size)))\n",
    "    self.coef_.append(np.random.randn(self.coef_[-1].shape[1],1)*np.sqrt(2.))\n",
    "    self.intercepts_.append(np.zeros((1)))\n",
    "    change_W.append(np.zeros(self.coef_[-1].shape))\n",
    "    change_b.append(np.zeros(1))\n",
    "\n",
    "    #Calculate batch_size\n",
    "    batch_size = min(200, x_train.shape[0])\n",
    "    n_batches = y_train.shape[0]//batch_size\n",
    "\n",
    "    for i_batch in range(n_batches):\n",
    "      Z = [0]*len(self.coef_)\n",
    "      A = [0]*len(self.coef_)\n",
    "      dZ = [0]*len(self.coef_)\n",
    "      dW = [0]*len(self.coef_)\n",
    "      db = [0]*len(self.coef_)\n",
    "\n",
    "      #Subset x_train and y_train for this batch\n",
    "      start_row = i_batch*batch_size\n",
    "      sub_x = x_train[start_row:start_row+batch_size]\n",
    "      sub_y = y_train[start_row:start_row+batch_size].reshape((-1,1))\n",
    "\n",
    "      #Forward propagation\n",
    "      Z[0] = np.matmul(sub_x,self.coef_[0]) + self.intercepts_[0]\n",
    "      A[0] = relu(Z[0])\n",
    "      for i in range(1,len(Z)):\n",
    "        Z[i] = np.matmul(A[i-1],self.coef_[i]) + self.intercepts_[i]                                                                                                                  \n",
    "        A[i] = relu(Z[i])\n",
    "\n",
    "      #Backward propagation\n",
    "      i = len(Z)-1\n",
    "      dZ[i] = A[i] - sub_y\n",
    "      dW[i] = np.matmul(A[i-1].T,dZ[i],)/batch_size\n",
    "      db[i] = np.sum(dZ[i],axis=1,keepdims=True)/batch_size\n",
    "      for i in range(len(Z)-2,0,-1):\n",
    "        dZ[i] = np.matmul(dW[i+1].T,dZ[i+1]) * relu(Z[i])\n",
    "        dW[i] = np.matmul(dZ[i],A[i].T)/batch_size\n",
    "        db[i] = np.sum(dZ[i],axis=1,keepdims=True)/batch_size\n",
    "      dZ[0] = np.matmul(dZ[1],dW[1].T) * relu(Z[0])\n",
    "      dW[0] = np.matmul(sub_x.T,dZ[0])/batch_size\n",
    "      db = np.sum(dZ[0],axis=1,keepdims=True)/batch_size\n",
    "\n",
    "      #Update weights and biases\n",
    "      for i in range(len(Z)):\n",
    "        change_W[i] = self.lern_rate*dW[i] + self.momentum*change_W[i]\n",
    "        self.coef_[i] -= change_W[i]\n",
    "        change_b[i] = self.lern_rate*db[i] + self.momentum*change_b[i]\n",
    "        self.intercepts_[i] -= change_b[i]\n",
    "\n",
    "  def predict(self, x_test: NumNPArrayNxM) -> ArrayLike:\n",
    "    H = np.matmul(x_test,self.coef_[0]) + self.intercepts_[0]\n",
    "    for i in range(1,len(self.coef_)):\n",
    "      H = relu(H)\n",
    "      H = np.matmul(H,self.coef_[i]) + self.intercepts_[i]\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7qI27SZ7BA4"
   },
   "source": [
    "Regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlTMKcqYjk88"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor as MLPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkqMkoSozwNQ"
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = process_gpu_running_time()\n",
    "\n",
    "print(\"My model:\")\n",
    "t0 = time()\n",
    "model = MLPRegressor()\n",
    "model.fit(x_train,y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"Elapse time = {:.5f}\".format(time() - t0))\n",
    "print(\"MSE = \", MSE(y_test, pred))\n",
    "\n",
    "print(\"\\nSKLEARN model:\")\n",
    "t0 = time()\n",
    "model = MLPR(hidden_layer_sizes=(100,),activation='relu',solver='sgd',learning_rate_init=0.001,momentum=0.9)\n",
    "model.fit(x_train,y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"Elapse time = {:.5f}\".format(time() - t0))\n",
    "print(\"MSE = \", MSE(y_test, pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
